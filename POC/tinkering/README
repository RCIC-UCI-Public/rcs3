# This is some tinkering to see what it takes to put some basic file system information into a sqlite3 database to then query
# Think of this as a poor-man's robinhood

# There are two methods of obtaining file/directory information and then loading into a database
#
#    1. rclone lsjson --metadata <path>
#    2. Download the inventory from S3 (generated by AWS nightly for an RCS3 backup)  
#
# Once you have data, it then needs to be processed (filtered) to create SQL inserts. 
# You should have a separate database for each rclone lsjson/inventory download and that DB should be empty
#
# The logical pipeline for both methods
#    ./createdb.py <sqlite.db>
#    rclone lsjson --metadata --exclude-from exclude-snaps <pathroot> | ./filterjson.py <pathroot> | sqlite3 <sqlite.db>
#                                                  OR
#    retrieve-manifest.py -o OWNER -s SYSTEM -c | gunzip -c | filtercsv.py | sqlite3 <sqlite.db>
#
# You can save the raw files and then "cat" them through the filter. The inventory files from AWS are already compressed. You
# should explicitly compress the output of lsjson by using gzip.
    
 
# ===============  Here's an example of an rclone capture =========
# if [ "x$TMPDIR" == "x" ]; then
#    TMPDIR=$(pwd)
# fi
# 
# DB=dfs4-bsg.db
# JSONFILE=$TMPDIR/dfs4-bsg.json.gz
# PATHINVENTORY=/dfs4/bsg
# BASEDB=$(basename $DB)
# 
# # Sample to create lsjson 
# if [ ! -f $JSONFILE ]; then
#    echo "Creating lsjon inventory of $PATHINVENTORY. Storing in File $JSONFILE"
#    (time rclone lsjson --exclude-from exclude-snaps --metadata --recursive $PATHINVENTORY | gzip -c --fast > $JSONFILE) &> $TMPDIR/$BASEDB-json.time 
# fi
# 
# # Create a new DB if needed
# if [ ! -f $DB ]; then
#    ./createdb.py $DB
# fi
# 
# # Ingesting lsjson output into a sqlite DB (DB should be on /dev/shm or on nvme)
# echo "Ingesting $JSONFILE into $DB.  Monitor progress in $TMPDIR/${BASEDB}-ingest.out"
# (time zcat $JSONFILE | ./filterjson.py $PATHINVENTORY | sqlite3 $DB) &> $TMPDIR/${BASEDB}-ingest.out &
# 

# ===============  Here's an example of an inventory download capture =========
#
# Assumes valid AWS credentials and permissions
#
# if [ "x$TMPDIR" == "x" ]; then
#    TMPDIR=$(pwd)
# fi
# 
# OWNER=rcic-admin
# SYSTEM=crsp
# DB=${OWNER}-${SYSTEM}.db
# BASEDB=$(basename $DB)
#
# # Create a new DB if needed
# if [ ! -f $DB ]; then
#    ./createdb.py $DB
# fi
#
# Download the files locally then ingest
# ./retrieve-manifest.py -o ${OWNER} -s ${SYSTEM} -D ${TMPDIR}/${OWNER}-${SYSTEM}
# 
# time (zcat ${TMPDIR?}/${OWNER}-${SYSTEM}/*.csv.gz | ./filtercsv.py | sqlite3 ${DB}) &> ${TMPDIR}/${OWNER}-${SYSTEM}-ingest.out &
#
# ===  OR ===  Stream download the manifest and ingest in one pipeline
#
# time (./retrieve-manifest.py -o ${OWNER} -s ${SYSTEM} -c | gunzip -c | ./filtercsv.py | sqlite3 ${DB}) &> ${TMPDIR}/${OWNER}-${SYSTEM}-ingest.out &


# Some interesting queries (If the data is large, like SOM, some of these queuries can take 1-2 minutes)
# find some counts
# 1. # of files
#    select count(*) from allfiles;
# 2. # of folders
#    select count(*) from folders;
# 3. Size and number of files in each top-level directory
#    select level1,sum(size) as bytes,count(*) as nfiles from allfiles group by level1;
# Dates are stored as Julian dates (real). Use datetime to format
# 4. Find the date of the newest file by mtime
#    select datetime(max(jmtime)) from allfiles;
# 5. Find the the date of the newest file by atime (accesstime)
#    select datetime(max(jatime)) from allfiles; 


## 
